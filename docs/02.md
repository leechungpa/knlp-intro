<center> 
    자연어처리 응용시스템
</center>
기계 번역, 정보 검색, 정보 추출, 음성 인식, 단어 분류, 구문 분석, 문장/문서 분류, 감정 분석, 의미역 결정, 자동 대화 시스템





# 개체명 인식 Named Entity Recognition

사람, 장소, 기관, 날짜, 학문적 관심분야(약물, 임상 절차 등) 등 명명된 개체를 인식하는 작업

- 질의 응답, 정보 검색, 공동 참조 해결, 토픽 모델링 등의 첫 단계로 사용
- NER 평가 척도로 F1 score 사용



### 개체명 인식 시스템

- 지도학습 기반 시스템
  - Hidden Markov Models
  - Support Vector Machines
  - Conditional Random Fields
- 지식 기반 시스템
- 비지도 및 부트스트랩 시스템



cf. BIO 태깅 기법 : 개체명을 텍스트로부터 인식시키기 위한 기법으로 정보 추출 작업에서 자주 이용되는 방법

- Begin는 개체명 중 시작하는 단어에 태그, Inside는 B 또는 I 다음의 개체명을 태그
- Outside는 개체명이 아닌 나머지 단어를 태그





# 언어 모델 Language Model

언어를 이루는 구성 요소(글자, 형태소, 단어, 문장, 문단 등)에 확률을 부여하여, 다음 요소를 예측 생성하는 모델



### 통계적 언어 모델 SLM

단어를 discrete symbol로 보기 때문에 기본적인 한계 존재 : 등장하지 않은 조합에 부족하며, Markov 가정에 의해 멀리 떨어진 단어를 반영하지 못함



- 조건부 확률을 이용한 모델

  - chain rule을 바탕으로 Markov 가정을 통해 계산량 줄임
  - 따라서 장기 의존성이 간과됨

- $N$-gram 언어 모델

  - 장기 의존성을 위하여 이전 뿐만 아니라, 주변 $N$개의 단어($N$-gram)를 고려

  - 따라서 $N$-gram은 $N-1$차 Markov assumption
    $$
    p(w_1,w_2,\cdots,w_n) \approx
    \prod_{i=1}^N p(w_i|w_{i-N},\cdots,w_{i-1})
    $$

  - $N$이 높아질 수록 코퍼스에 존재하는 텍스트에 가까운 새롭지 않은 문장이 생성 ⇒ 일반적으로 $N=3$

  - 테스트 데이터셋과 학습셋이 비슷해야 좋은 결과를 보임

  - sparsity problem :  카운트 기반 모델은 코퍼스에 특정 단어열이 없는 경우 확률이 0이 됨

  - 한국어 모델 : 조사 등의 이유로 형태소 분석 및 토큰화 같은 전처리를 우선적으로 필요



- 로그 확률 log probabilities
  - N-gram 모델의 경우 매우 작은 확률값을 많이 곱하기에 underflow 발생 가능
  - 따라서 일반적으로 log probabilities 사용



- Generalized method for sparsity problem

  - Smoothing : 코퍼스에 존재하지 않는 단어 조합에 특정 값($0<\alpha \leq1$)을 부여하여, 없는 단어에 대한 확률이 0이 되는 것을 방지
    $$
    p(w_i |w_{i-1},w_{i-2},\cdots) \approx
    {count(w_i,w_{i-1},w_{i-2})+\alpha /|V| \over count(w_{i-1},w_{i-2})+\alpha}
    $$

    - $|V|$ : a size of vocabulary (코퍼스에 등장하는 단일 단어의 개수
    - $1/|V|$ 대신 단어가 나타난 확률인 $P(w_t)$로 일반화 가능 

  - Interpolation : $N$-gram 확률 계산시 ($N-1$)-gram, ($N-2$)-gram 등 이전의 확률도 가중치를 곱하여 반영하는 방법

    - domain specific LM와 general domain LM를 interpolation할 수도 있음
  
  - Back off : $N$-gram 확률 계산시 ($i$)-gram을 사용하고, 만약 그 값이 0이라면 ($i-1$)-gram 을 이용하는 방법



### 딥러닝 언어 모델 DNN LM

SLM방식의 한계를 극복





### 언어모델 평가 지표

Perplexity : 주어진 확률 모델(LM)이 샘플을 얼마나 헷갈리게 예측하는 정도 (예측하는 평균적인 branch 수)
$$
PPL(W) = p(w_1,w_2,\cdots,w_n)^{-1/N}
\approx
[\prod_{i=1}^N p(w_i|w_{i-1})]^{-1/N}
$$

- $W$ : $N$개의 단어 $w_i$로 이루어진, 모델을 통해 예측한 문장
- PPL 계산시 확률은 앞서 언급한 일반화 및 log 기법이 사용된 확률
- 잘 정의된 문장에 대해서 높은 확률을 반환할 수록 좋은 모델이므로, PPL이 낮을 수록 좋음





# 정보 추출 Information Extraction

비정형 텍스트로부터 구조화된 유용한 정보를 자동으로 추출하는 것

- 정보 추출의 목적 : 문서 내 단어 간의 대상 관계를 파악하여 의미적 관계를 추출하고 이에 대해 응답을 하는 것
- 방법 : 텍스트 ⇒ 문장 분류, 토큰화, 품사 태깅 ⇒ NER 활용하여 분류 ⇒ 엔티티 및 관계를 파악



cf. 상호 참조 co-reference resolution

	-  이전 단계에서 추출된 엔티티의 유형을 기반으로 엔티티 간의 상호 참조 및 위키피디아 링크를 찾아 찾음





# 질의 응답 Question & Answering

### 정보 검색 기반 질의응답 Information retrieval-based Q&A

- 전통적인 방법
- 처리 과정 : 질문 처리 ⇒ 문서 처리 ⇒ 정답 처리
  - 질문 처리 단계 : 형태소 분석, 개체명 인식, 의문사 어절 어휘의미 정보 등을 활용하여 **질문유형 분류**와 **정답유형 분류**
  - 문서처리 단계 : 주어진 질의와 가장 관련 있는 문서(또는 문서 내의 문장)을 찾음
  - 정답 처리 단계 : 검색된 문서나 문장에서 정답 후보에 해당하는 개체 어휘 구를 추출하는 것



### 지식 기반 질의응답 Knowledge-based Q&A

- 딥러닝을 이용한 방법 등



### 문서 검색 모델

- Boolean model : 출현 여부에 따라 1 또는 0으로 표현하여 모델
- Vector space model : 각 문서에 나오는 단어들에 대하여 가중치를 측정하여, 그에 따라 질의어와 문서의 유사도를 측정하여 각 문서의 순위를 나타낼 수 있다.



### 문서간의 유사도 측정방법

- Jaccard similarity

  - 두 집합의 합집합에서 교집합의 비율

    
    $$
    J(A,B)={ |A\cap B| \over |A\cup B|}
    $$

  - 두 집합이 동일하면 1, 공통원소가 없으면 0

  - TF와 DF를 고려하지 않음

- 코사인 유사도

- TF-IDF : TF와 IDF의 곱으로 클수록 중요도가 큰 것

  - 단어 빈도수 Term Frequency : $tf(d,t)$
    - 단어 $t$가 특정 문서$d$에 나타난 횟수
    - boolean model과 비슷하나, 출현 여부가 아닌 출현횟수를 나타냄
  - 문서 빈도수 Document Frequency : $df(t)$
    - 단어 $t$가 나타난 문서의 수
    - DF가 높으면 여러 문서에서 나왔다는 의미로 중요도가 떨어지는 것
  - Inverse Dcoument Frequency : $idf(d,t)=log({n\over 1+df(t)})$
    - 전체 단어수 $n$을 해당 단어의 $df(t)$로 나눈 후 $log$ 취한 값
    - 즉 IDF가 클수록 중요







# 기계번역

- 규칙기반 기계번역 : 최근에는 사용하지 않음
- 통계기반 기계번역 : 대량의 병렬 코퍼스를 바탕으로 두 언어 사이의 상관관계를 통계적으로 분석하여 모델 생성
- 딥러닝기반 기계번역 : 통계기반 기계번역을 대량의 신경망으로 확대한 것



구 기반 번역 Phrase-Based Model : 단어 기반으로 번역시 발생하는 문제를 보완





# 자연어 생성

초기의 자연어 생성은N-gram 모델에 바탕하였으나, 최근에는 이를 확장한 인공 신경망 언어 모델 NNLM 등이 사용



### 지도학습 기반 자연어 생성

- MLE : 자연어 생성 문제를 순차적 다중 레이블 분류로 간주하여 cross entropy를 직접 최적화
- 순환 신경망 언어 모델 RNNLM



auto-regressive 생성 모델은 학습 환경과 추론 환경간의 불일치 현상으로 노출 편향 exposure bias 발생 ⇒ 노출 편향을 줄이기 위한 방안

- Scheduled sampling : 자동-회귀적 생성 모델이 학습 과정에서도 추론 환경을 미리 경험하도록 학습하여 학습 과정과 추론 과정의 차이를 줄임
- Transformer : RNN가 다르게 입력을 병렬로 처리하여 은닉 상태로 표현



### 강화학습 기반 자연어 생성

- PG-BLEU
- 모방 학습 Imitation Learning



### 적대학습 기반 자연어 생성

- Generative Adversarial Networks
- Sequence GAN





# 대화 시스템 Dialog System

종류 : 사용자 주도 대화 시스템, 시스템 주도 대화 시스템

과정 : 음성인식 → 자연어 이해 → 대화관리 ( ↔ 작업 관리 ) → 자연어 생성 → 음성 합성



대화관리 시스템의 종류 : 규칙기반 접근법, 데이터 기반 접근법



대회사스템의 목적에 따른 분류 : 기능대화(수시로 바뀌는 domain-specific한 목적이 있음), 일상대화(대화 자체가 목적)







# 문서 요약 Text Summarization

문서 요약 단계

- Interpretation : 텍스트를 컴퓨터가 이해 가능하게 표현하는 단계
- Transformation : 요약문으로 표현되도록 문서를 변형 및 가공하는 단계
- Generation : 최종 요약문을 생성하는 단계



문서요약 평가 방법 : ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

​	정답 요약(reference summary)과 모델이 생성한 요약을 비교하여 자동적으로 요약 시스템의 성능을 측정



### 통계적 접근법

문서의 주요 주제나 사용자의 요청 사항에 대해 일부 자질들을 이용하여 텍스트간(일반적으로 문장 단위)의 연관성 점수를 계산하는 방식으로 요약문 구성

- 용어 빈도 term frequency : tf-idf등을 활용

- 텍스트의 위치 : 단어의 위치에 따라 단어가 중요한 정도가 다르다는 가정에서 시작 → 가정에 따라 다르게 정의한 함수 $pos(w_i)$ 사용 (일반적으로 앞에 나올 수록 더 중요한 의미를 뜻함)
  $$
  \begin{align}
  Score(s)=
  \sum_{w_i \in s}{log(freq(w_i))pos(w_i)\over |s|}
  \qquad \text{where}\quad
  &freq(w_i)\text{ : 단어}\ w_i\text{의 길이}
  \\
  &|s|\text{ : 문장의 길이}
  \end{align}
  $$
  

  - Direct Proportion : 처음 출현하면 $1$이고 마지막에 출현하면 $1/n$
  - Inverse Proportion : $1/i$ where $i$ 는 위치 번호 (선행되는 문장의 위치가 유리하며 DP의 반대 개념)
  - Geometric Sequence : 모든 단어에 대해 $(1/2)^{i-1}$로 도출된 값들의 합을 사용
  - Binary Function : 단어의 첫 등장에 더 중요한 가중치를 부여

- 잠재 의미 분석 Latent Semantic Analysis

  - 문서내의 용어들을 행으로 문장을 열로 구성한 행렬을 만들어, singular value decomposition를 이용하여 문장의 중요도 계산
  - cf. https://wikidocs.net/24949



### 기계학습 접근법

다른 접근법들과 같이 사용하여 성능을 향상시키는 방향으로 활용



결국 문서 요약은 문장이 요양문에 포함될지 아닐지를 결정하는 분류의 문제

- 베이지안 분류





# 감성 분석 Sentiment Analysis

텍스트 분류 프로세스

- 데이터 준비 및 전처리 : 불용어 제거 등
- 토큰화 및 특징 값 추출 : TFIDF등 목적에 따라 다양한 특징값 사용
- 모델링 : SVM, ANN, Decision Tree 등 분류 모델 사용
- 모델 평가 및 개선



