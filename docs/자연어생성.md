 <center> 딥러닝을 활용한 자연어생성 </center>

목표 : 컴퓨터가 인간이 만들어놓은 대량의 문서를 통해 정보를 얻고(NLU), 얻어낸 정보를 사람이 이해할 수 있게 사람의 언어로 표현하는 것(NLG)

# Introduction

### Perspective of Deeplearning

 : 세상에 존재하는 어떤 미지의 확률 분포 함수를 모사(approximate)

​    

Probabilistic Perspective

- 확률 분포 𝑃(𝑥)와 𝑃(𝑦|𝑥)로부터 데이터를 수집하여, 해당 데이터를 가장 잘 설명하는 모수𝜃 찾는 것
   Maximum Likelihood Estimation
   Gradient Descent using Back-propagation

- 또는 두 확률 분포를 비슷하게 만드는 것
   Minimize Cross Entropy (or KL-Divergence)

​    

Geometric perspective

- 데이터란 저차원의 manifold에 분포하고 있으며, 여기에 약간의 노이즈(𝜖)가 추가 되어 있는 것
   노이즈란 task(𝑥 → 𝑦)에 따라서 다양하게 해석 가능 할 것

- 따라서 해당 manifold를 배울 수 있다면, 더 낮은 차원으로 효율적인 맵핑(or project)이 가능
   Non-linear dimension reduction

​    

Representation Learning, Again

- 낮은 차원으로의 표현을 통해, 차원의 저주(curse of dimensionality)를 벗어나 효과적인 학습이 가능

​    

​    

### Review of NLP

Autoencoders

- 인코더(encoder)와 디코더(decoder)를 통해 압축과 해제를 실행
   인코더는 입력(𝑥)의 정보를 최대한 보존하도록 손실 압축을 수행
   디코더는 중간 결과물(𝑧)의 정보를 입력(𝑥)과 같아지도록 압축 해제(복원)를 수행

- 복원을 성공적으로 하기 위해, 오토인코더(autoencoder)는 **특징(feature)을 추출하는 방법을 자동으로 학습** 

​    

Word2Vec

- 주어진 단어로 주변 단어를 예측하는 것이 목적
- Autoencoder의 개념과 비슷
- 𝑦를 예측하기 위해 필요한 정보가 𝑧에 있어야 함 : 주변 단어를 잘 예측하기 위해 **𝑥를 잘 압축**해야 함.





Wrap-up

- 신경망은 𝑥와 𝑦 사이의 관계를 학습하는 과정에서 feature를 자연스럽게 학습 : 특히 저차원으로 축소(압축)되는 과정에서 정보의 취사/선택이 이루어짐

- Worde Embedding (Skip-gram) : 주변 단어(𝑦)를 예측하기 위해 필요한 정보를 현재 단어(𝑥)에서 추출하여 압축

- Sentence Embedding (text classification) : Label(𝑦)을 예측하기 위해 필요한 정보를 단어들의 시퀀스(𝑥)로부터 추출하여 압축

​    

# 언어 모델 Language Model

문장의 확률을 나타낸 모델 수식
$$
D = \{ w_1, w_2, \cdots , w_N \}
\\
\hat{\theta}
 = \operatorname*{argmax}_{\theta \in \Theta } \sum^N_{i=1}\log P(x^i_{1:n};\theta)
 = \operatorname*{argmax}_{\theta \in \Theta } \sum^N_{i=1}\sum^n_{j=1}\log P(x^i_j|x^i_{1:j};\theta)
\\
\hat{x_t} = \operatorname*{argmax}_{x_t\in X} \log P(x_t|x_{1:t};\theta)
$$

- 문장 자체의 출현 확률을 예측 하거나, 이전 단어들이 주어졌을 때 다음 단어를 예측하기 위한 모델
- 코퍼스를  바탕으로 단어와 단어 사이의 출현 빈도를 세어 확률을 계산
- 궁극적인 목표는 우리가 일상 생활에서 사용하는 **언어의 문장 분포**를 정확하게 모델링 하는 것

- minimize perplexity = maximize likelihodd : 따라서 언어모델링은 주어진 단어가 있을 때, 다음 단어의 likelihood를 최대화하는 파라미터를 찾는 과정





